{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ojuERcus61hu",
        "I4eUfYk_oQi8",
        "yojwXG96yFr_",
        "FwKlZui2ypR3"
      ],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "# TorchVision Finetuning\n",
        "\n",
        "I have used the code and DataSet from the tutorial for my option 2."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "fPDbOPEm7om_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDb7PBw55b-"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.8.2\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4TBwhHTdkd"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# download the Penn-Fudan dataset\n",
        "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "# extract it in the current folder\n",
        "unzip PennFudanPed.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIoe_tHTQgV"
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjNHjVMOyYlH"
      },
      "source": [
        "      \n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Av5O2LC940aD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u9e_pdv54nG"
      },
      "source": [
        "\n",
        "\n",
        "As mentioned in the tutorial, I am making use of the references helpers from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l79ivkwKy357"
      },
      "source": [
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5dGaIezze3y"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5yvZUprj4ZN"
      },
      "source": [
        "Now , the option 1 model is instantiated here and Optimiser will also be defined "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoenkCj18C4h"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd56lt4kDxc"
      },
      "source": [
        "The training is done for 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at-h4OWK0aoc"
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer(a)\n",
        "\n",
        "Using the option 2 for my backbone as  `model2`."
      ],
      "metadata": {
        "id": "I4eUfYk_oQi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_modified_model(num_classes):\n",
        "  backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "  backbone.out_channels = 1280\n",
        "  \n",
        "  anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "  \n",
        "  roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\"],\n",
        "                                                  output_size=7,\n",
        "                                                  sampling_ratio=2)\n",
        "  \n",
        "  return FasterRCNN(backbone,\n",
        "                    num_classes=num_classes,\n",
        "                    rpn_anchor_generator=anchor_generator,\n",
        "                    box_roi_pool=roi_pooler)\n",
        "\n",
        "model2 = get_modified_model(num_classes=2)"
      ],
      "metadata": {
        "id": "AWKx7Ff4kaVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True,\n",
        "                                         num_workers=4, collate_fn=utils.collate_fn)\n",
        "\n",
        "images, targets = next(iter(dataloader))\n",
        "\n",
        "model2.train()\n",
        "images = list(images)\n",
        "targets = [{k: v for k,v in t.items()} for t in targets]\n",
        "output = model2(images, targets)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "LCLhr5kYo8qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.to(device)\n",
        "\n",
        "params = [p for p in model2.parameters() if p.requires_grad]\n",
        "optimizer2 = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "\n",
        "lr_scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "metadata": {
        "id": "Oun-ydWQp0Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_one_epoch(model2, optimizer2, dataloader, device, epoch, print_freq=10)\n",
        "  lr_scheduler2.step()"
      ],
      "metadata": {
        "id": "E2bXn3J-qe8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer(b)\n",
        "\n",
        "Comparision of both models (`model` and `model2`) on  training image.\n",
        "\n",
        "Plotting the image and the boundary boxes which will be defined as: x,y,width and height\n",
        "I will compare by imposing the boundary box on the images and I have used ChatGPT and plotting function code from kaggle: https://www.kaggle.com/code/kheyduzoomryou/fine-tuning-faster-rcnn-using-pytorch-aca144"
      ],
      "metadata": {
        "id": "yojwXG96yFr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_img_bbox(image, boxes):\n",
        "    \n",
        "    fig, a = plt.subplots()\n",
        "    a.imshow(image)\n",
        "    for box in boxes:\n",
        "        x, y, width, height = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "        rect = patches.Rectangle((x, y),\n",
        "                                 width, height,\n",
        "                                 linewidth = 2,\n",
        "                                 edgecolor = 'r',\n",
        "                                 facecolor = 'none')\n",
        "\n",
        "        \n",
        "        a.add_patch(rect)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "H6DjAHjdyPB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, _ = dataset[0]\n",
        "img = image.to(device)"
      ],
      "metadata": {
        "id": "DskztFbmyxdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_image = Image.fromarray((255*image).numpy().astype(np.uint8).transpose(1, 2, 0))\n",
        "\n",
        "model.eval()\n",
        "boxes = model([img])[0]['boxes'].cpu().detach().numpy()\n",
        "plot_img_bbox(modified_image, boxes[:2])\n"
      ],
      "metadata": {
        "id": "n4catakZ2mfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.eval()\n",
        "boxes2 = model2([img])[0]['boxes'].cpu().detach().numpy()\n",
        "plot_img_bbox(modified_image, boxes2[:2])"
      ],
      "metadata": {
        "id": "I0sX5mlry_3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "0QWIX2Cm5yzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer(c)\n",
        "\n",
        "Now we compare the models on a Beatles image. I used ChatGPT  to get an image url and response"
      ],
      "metadata": {
        "id": "FwKlZui2ypR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "url = \"https://upload.wikimedia.org/wikipedia/en/4/42/Beatles_-_Abbey_Road.jpg\"\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content))"
      ],
      "metadata": {
        "id": "o72qf1s7q3f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = torchvision.transforms.ToTensor()(\n",
        "    torchvision.transforms.ToPILImage()(\n",
        "        np.array(image)\n",
        "    )\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "x7ugUIpYsNVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "prediction_beatles = model([img])[0]['boxes'].detach().cpu().numpy()\n",
        "plot_img_bbox(image, prediction_beatles[:4])"
      ],
      "metadata": {
        "id": "y6hJsx0X3VzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.eval()\n",
        "prediction_beatles2 = model2([img])[0]['boxes'].detach().cpu().numpy()\n",
        "plot_img_bbox(image, prediction_beatles2[:4])"
      ],
      "metadata": {
        "id": "KBZikix_sp75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The option 1 performs significantly better than the option 2 with boundary boxes estimation of objects."
      ],
      "metadata": {
        "id": "RCFB5CP76F8p"
      }
    }
  ]
}